{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNggMfJAETEbzco1r4oNBAh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquibjaved/Bits_and_Pieces_DL/blob/main/Basics_of_CUDA_with_numba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvqzc_x6QeTq",
        "outputId": "893f50bc-8b2a-4279-bde0-5c2f177b194a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "metadata": {
        "id": "BwAJCqx2Xoqk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDA Kernel is function that will be called from CPU\n",
        "\n",
        "\n",
        "*   **Kernels can't return a value:** A kernel function (in GPU programming) can't\n",
        "    directly return a result. Instead, it writes its results to an array that you pass to it. If the result is just one value (a scalar), you'd use a one-element array to store that value.\n",
        "\n",
        "*   **Thread hierarchy in kernels:** When calling a kernel, you must specify how many thread blocks (groups of threads) and how many threads per block it will use. Although the kernel code is compiled once, you can call it multiple times with different numbers of blocks or threads, depending on the task.\n"
      ],
      "metadata": {
        "id": "Fa6Lv0eQRyxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDA **thread** are the single unit of computation: It represent a smallest independent task, a single thread process specific data element and many work together to bring parallelism in massive amount.\n",
        "\n",
        "Key Points about Threads:\n",
        "\n",
        "* Single task: Each thread runs a copy of the same kernel code but operates on different data (based on its unique thread ID).\n",
        "\n",
        "* Parallel execution: Thousands of threads can run simultaneously on the GPU, allowing massive parallelism.\n",
        "\n",
        "* Thread IDs: Threads are uniquely identified within their block (using threadIdx.x, threadIdx.y, threadIdx.z), and you can use this ID to determine which part of the data the thread processes.\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "# Define the CUDA kernel in Numba\n",
        "@cuda.jit\n",
        "def add_arrays(a, b, result, size):\n",
        "    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x  # Calculate thread's global index\n",
        "    if idx < size:\n",
        "        result[idx] = a[idx] + b[idx]  # Perform the addition\n",
        "\n",
        "# Initialize input arrays\n",
        "size = 100\n",
        "a = np.random.randint(0, 100, size).astype(np.int32)  # Random integers array\n",
        "b = np.random.randint(0, 100, size).astype(np.int32)\n",
        "result = np.zeros(size, dtype=np.int32)  # Output array\n",
        "\n",
        "# Copy data to the GPU\n",
        "d_a = cuda.to_device(a)\n",
        "d_b = cuda.to_device(b)\n",
        "d_result = cuda.to_device(result)\n",
        "\n",
        "# Define number of threads and blocks\n",
        "threads_per_block = 32\n",
        "blocks_per_grid = (size + (threads_per_block - 1)) // threads_per_block  # Ensure full coverage\n",
        "\n",
        "# Launch the kernel\n",
        "add_arrays[blocks_per_grid, threads_per_block](d_a, d_b, d_result, size)\n",
        "\n",
        "# Copy the result back to the CPU\n",
        "result = d_result.copy_to_host()\n",
        "\n",
        "# Display the result\n",
        "print(result)\n",
        "\n",
        "```\n",
        "\n",
        "1. Threads per block (threads_per_block = 32):\n",
        "This defines how many threads you want in each block. In this case, we are assigning 32 threads to each block.\n",
        " The maximum number of threads per block typically ranges from 32 to 1024, depending on the GPU.\n",
        "Choosing 32 threads is a common choice because it's the size of a \"warp\" on most GPUs, which is a group of threads that execute together in lockstep (for efficiency).\n",
        "2. Blocks per grid (blocks_per_grid = (size + (threads_per_block - 1)) // threads_per_block):\n",
        "**This calculates the number of blocks needed to ensure that every element of the array is processed by a thread.**\n",
        "\n",
        "Breakdown of the formula:\n",
        "\n",
        "**size** : This is the total number of elements in the arrays a, b, and result that need to be processed (in this case, 100 elements).\n",
        "\n",
        "**threads_per_block - 1** : This is added to the array size to handle cases where size isn’t a perfect multiple of threads_per_block. Adding (threads_per_block - 1) ensures that any remaining elements in the array (if size isn't divisible by threads_per_block) are still covered by an additional block.\n",
        "\n",
        "***// threads_per_block: This is integer division. It determines how many full blocks are needed. For example:***\n",
        "\n",
        "If size = 100 and threads_per_block = 32, then:$$\n",
        "\\text{blocks_per_grid} = \\frac{100 + 31}{32} = \\frac{131}{32} = 4 \\text{ blocks}\n",
        "$$ This ensures there are enough blocks to cover all elements. In this example, 4 blocks of 32 threads will provide coverage for 128 total threads, which is more than enough to process all 100 elements.\n",
        "Why This Formula Is Used:\n",
        "CUDA organizes threads into blocks. If the size of the data (size = 100) is not divisible by the number of threads per block (threads_per_block = 32), there will be leftover elements that won’t get processed unless an additional block is added. The formula ensures that even if there are leftover elements, they will be covered by an extra block.\n",
        "\n",
        "This approach guarantees full coverage of the data, even when the number of elements doesn't perfectly fit into the blocks.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "**threads_per_block** specifies how many threads each block has (32 here).\n",
        "\n",
        "**blocks_per_grid** calculates how many blocks are needed to make sure every element in the array gets processed, ensuring that no data is left out.\n"
      ],
      "metadata": {
        "id": "FtBw62eJU5Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dX-k3ms3STJA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}