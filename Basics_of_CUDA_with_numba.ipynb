{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+wLS9vSsMAeN0v9p9PO5I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquibjaved/Bits_and_Pieces_DL/blob/main/Basics_of_CUDA_with_numba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvqzc_x6QeTq",
        "outputId": "f439449b-b3f8-4058-fce8-a50f75271fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "metadata": {
        "id": "BwAJCqx2Xoqk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDA Kernel is function that will be called from CPU\n",
        "\n",
        "\n",
        "*   **Kernels can't return a value:** A kernel function (in GPU programming) can't\n",
        "    directly return a result. Instead, it writes its results to an array that you pass to it. If the result is just one value (a scalar), you'd use a one-element array to store that value.\n",
        "\n",
        "*   **Thread hierarchy in kernels:** When calling a kernel, you must specify how many thread blocks (groups of threads) and how many threads per block it will use. Although the kernel code is compiled once, you can call it multiple times with different numbers of blocks or threads, depending on the task.\n"
      ],
      "metadata": {
        "id": "Fa6Lv0eQRyxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUDA **thread** are the single unit of computation: It represent a smallest independent task, a single thread process specific data element and many work together to bring parallelism in massive amount.\n",
        "\n",
        "Key Points about Threads:\n",
        "\n",
        "* Single task: Each thread runs a copy of the same kernel code but operates on different data (based on its unique thread ID).\n",
        "\n",
        "* Parallel execution: Thousands of threads can run simultaneously on the GPU, allowing massive parallelism.\n",
        "\n",
        "* Thread IDs: Threads are uniquely identified within their block (using threadIdx.x, threadIdx.y, threadIdx.z), and you can use this ID to determine which part of the data the thread processes.\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "# Define the CUDA kernel in Numba\n",
        "@cuda.jit\n",
        "def add_arrays(a, b, result, size):\n",
        "    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x  # Calculate thread's global index\n",
        "    if idx < size:\n",
        "        result[idx] = a[idx] + b[idx]  # Perform the addition\n",
        "\n",
        "# Initialize input arrays\n",
        "size = 100\n",
        "a = np.random.randint(0, 100, size).astype(np.int32)  # Random integers array\n",
        "b = np.random.randint(0, 100, size).astype(np.int32)\n",
        "result = np.zeros(size, dtype=np.int32)  # Output array\n",
        "\n",
        "# Copy data to the GPU\n",
        "d_a = cuda.to_device(a)\n",
        "d_b = cuda.to_device(b)\n",
        "d_result = cuda.to_device(result)\n",
        "\n",
        "# Define number of threads and blocks\n",
        "threads_per_block = 32\n",
        "blocks_per_grid = (size + (threads_per_block - 1)) // threads_per_block  # Ensure full coverage\n",
        "\n",
        "# Launch the kernel\n",
        "add_arrays[blocks_per_grid, threads_per_block](d_a, d_b, d_result, size)\n",
        "\n",
        "# Copy the result back to the CPU\n",
        "result = d_result.copy_to_host()\n",
        "\n",
        "# Display the result\n",
        "print(result)\n",
        "\n",
        "```\n",
        "\n",
        "1. **Threads per block (threads_per_block = 32):**\n",
        "This defines how many threads you want in each block. In this case, we are assigning 32 threads to each block.\n",
        " The maximum number of threads per block typically ranges from 32 to 1024, depending on the GPU.\n",
        "Choosing 32 threads is a common choice because it's the size of a \"warp\" on most GPUs, which is a group of threads that execute together in lockstep (for efficiency).\n",
        "2. **Blocks per grid (blocks_per_grid = (size + (threads_per_block - 1)) // threads_per_block):**\n",
        "**This calculates the number of blocks needed to ensure that every element of the array is processed by a thread.**\n",
        "\n",
        "Breakdown of the formula:\n",
        "\n",
        "**size** : This is the total number of elements in the arrays a, b, and result that need to be processed (in this case, 100 elements).\n",
        "\n",
        "**threads_per_block - 1** : This is added to the array size to handle cases where size isn’t a perfect multiple of threads_per_block. Adding (threads_per_block - 1) ensures that any remaining elements in the array (if size isn't divisible by threads_per_block) are still covered by an additional block.\n",
        "\n",
        "***// threads_per_block: This is integer division. It determines how many full blocks are needed. For example:***\n",
        "\n",
        "If size = 100 and threads_per_block = 32, then:$$\n",
        "\\text{blocks_per_grid} = \\frac{100 + 31}{32} = \\frac{131}{32} = 4 \\text{ blocks}\n",
        "$$ This ensures there are enough blocks to cover all elements. In this example, 4 blocks of 32 threads will provide coverage for 128 total threads, which is more than enough to process all 100 elements.\n",
        "Why This Formula Is Used:\n",
        "CUDA organizes threads into blocks. If the size of the data (size = 100) is not divisible by the number of threads per block (threads_per_block = 32), there will be leftover elements that won’t get processed unless an additional block is added. The formula ensures that even if there are leftover elements, they will be covered by an extra block.\n",
        "\n",
        "This approach guarantees full coverage of the data, even when the number of elements doesn't perfectly fit into the blocks.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "**threads_per_block** specifies how many threads each block has (32 here).\n",
        "\n",
        "**blocks_per_grid** calculates how many blocks are needed to make sure every element in the array gets processed, ensuring that no data is left out.\n"
      ],
      "metadata": {
        "id": "FtBw62eJU5Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def add_arrays(a, b, result, size):\n",
        "    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x  # Calculate thread's global index\n",
        "    if idx < size:\n",
        "        result[idx] = a[idx] + b[idx]  # Perform the addition"
      ],
      "metadata": {
        "id": "dX-k3ms3STJA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "A thread can hold **multiple vairbales** and **manupulate** them.\n",
        "\n",
        "`cuda.threadIdx.x:` Index of the thread within its block.\n",
        "\n",
        "`cuda.blockIdx.x:` Index of the block within the grid.\n",
        "\n",
        "`cuda.blockDim.x:` Total number of threads per block.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This expression calculates the global index (idx) for the thread, allowing it to uniquely identify which data element to process by combining the thread's index within its block and the total number of threads in all previous blocks.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "For Thread 0 in Block 1 with `blockDim.x = 32`:\n",
        "\n",
        "`idx = 0 + (1 * 32) = 32` This means it processes the 33rd element of the data."
      ],
      "metadata": {
        "id": "OBBP7Sv9alMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize input arrays\n",
        "size = 100\n",
        "a = np.random.randint(0, 100, size).astype(np.int32)\n",
        "b = np.random.randint(0, 100, size).astype(np.int32)\n",
        "result = np.zeros(size, dtype=np.int32)  # Output array\n",
        "\n",
        "# Copy data to the GPU\n",
        "d_a = cuda.to_device(a)\n",
        "d_b = cuda.to_device(b)\n",
        "d_result = cuda.to_device(result)\n",
        "\n",
        "# Define number of threads and blocks\n",
        "threads_per_block = 32\n",
        "blocks_per_grid = (size + (threads_per_block - 1)) // threads_per_block  # Ensure full coverage\n",
        "\n",
        "print(f\"Number of threads per block: {blocks_per_grid}\")\n",
        "print(f\"Number of blocks needed to fit all the element of data: {threads_per_block}\")\n",
        "print(f\"Total number of threads: {blocks_per_grid * threads_per_block}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu--AfpLY6rm",
        "outputId": "07482b64-ccc8-45b0-8adf-fda83eab3d3a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of threads per block: 4\n",
            "Number of blocks needed to fit all the element of data: 32\n",
            "Total number of threads: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the kernel\n",
        "add_arrays[blocks_per_grid, threads_per_block](d_a, d_b, d_result, size)\n",
        "\n",
        "# Copy the result back to the CPU\n",
        "result = d_result.copy_to_host()\n",
        "\n",
        "# Display the result\n",
        "print(result)\n",
        "print(f\"Len of result : {len(result)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f19fA23_ZERQ",
        "outputId": "ffba58d7-e5d7-4938-91d4-737cab590841"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 39  74  64  75  67 148 113 146 101 166  88  69 150 115  27 109  85 100\n",
            " 139  46 122 143 120  60  69 137  68 109  81  65  96  49  64  91 179 107\n",
            "  43  81 125 126  86 165 100  96  84  92  52 166 159  93   4  75  28  99\n",
            " 113  62  75  43 133 113 127 128  96  65  63  55  93  50  53 122  69  16\n",
            "  73  83 125  28  98 147 104 138 100 177 154  80 157  60  52 145  25  74\n",
            " 136 103 112  14  85 110 170  20 120 148]\n",
            "Len of result : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "add_arrays[blocks_per_grid, threads_per_block](d_a, d_b, d_result, size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsys7da6Zi5x",
        "outputId": "31cce544-789e-4a0c-dd93-af66e7aa123d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67.9 µs ± 15.8 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-2qgsEHna-p_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}