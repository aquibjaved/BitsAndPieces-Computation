{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlP+VlZ2jDq24RMBXYixIi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquibjaved/Bits_and_Pieces_DL/blob/main/Basics_of_Knowledge_distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6BlpRWPLD7uv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.parametrize as P\n",
        "from sklearn.datasets import load_iris\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform to normalize the data and flatten each image into a 1D array\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    data, target = zip(*batch)\n",
        "    data = torch.stack(data)\n",
        "    target = torch.tensor(target)\n",
        "    return data.cuda(), target.cuda()\n",
        "\n",
        "train_dataset = datasets.FashionMNIST('~/.data', download=True, train=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST('~/.data', download=True, train=False, transform=transform)\n",
        "def split_dataset(dataset, labels):\n",
        "    indices = [i for i, (data, label) in enumerate(dataset) if label in labels]\n",
        "    return torch.utils.data.Subset(dataset, indices)\n",
        "\n",
        "labels_1 = [0, 1, 2, 3, 4]\n",
        "labels_2 = [5, 6, 7, 8, 9]\n",
        "\n",
        "train_dataset_04 = split_dataset(train_dataset, labels_1)\n",
        "train_dataset_59 = split_dataset(train_dataset, labels_2)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader_1 = torch.utils.data.DataLoader(train_dataset_04, batch_size=batch_size, shuffle=True)\n",
        "train_loader_2 = torch.utils.data.DataLoader(train_dataset_59, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3ut9pSwEJSg",
        "outputId": "b90bc53e-3327-431d-d184-b0554e90f964"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 18791995.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 306636.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 5567044.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 12226657.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionNet(nn.Module):\n",
        "  def __init__(self, input_dims, num_class, input_hidden_size: 1000, hidden_layer_size:2000):\n",
        "\n",
        "    super().__init__()\n",
        "    self.input_dims = input_dims\n",
        "    self.num_class = num_class\n",
        "    self.input_hidden_size= input_hidden_size\n",
        "    self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "\n",
        "    self.input_layer = nn.Linear(in_features=self.input_dims, out_features=self.input_hidden_size)\n",
        "    self.hidden_layer = nn.Linear(in_features=self.input_hidden_size, out_features=self.hidden_layer_size)\n",
        "    self.output_layer = nn.Linear(in_features=self.hidden_layer_size, out_features=self.num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.input_layer(x))\n",
        "    out = F.relu(self.hidden_layer(out))\n",
        "    out = self.output_layer(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "SVbksRWyEMki"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_available = torch.cuda.is_available()\n",
        "\n",
        "# Define the class names for Fashion MNIST\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "BigNet = FashionNet(input_dims=784, num_class=len(class_names), input_hidden_size=1000, hidden_layer_size=2000)\n",
        "optimizer = optim.SGD(BigNet.parameters(), lr=0.0001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Assume `model` is your model\n",
        "if cuda_available:\n",
        "    BigNet = BigNet.to('cuda')\n",
        "    optimizer = optim.SGD(BigNet.parameters(), lr=0.011, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "\n",
        "is_on_gpu = next(BigNet.parameters()).is_cuda\n",
        "print(f'Model is on GPU: {is_on_gpu}')\n",
        "\n",
        "trainable_params = sum(p.numel() for p in BigNet.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable (tunable) parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s74JxgsEfuD",
        "outputId": "e6592210-9f94-4d24-b994-3898461aab91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is on GPU: False\n",
            "Number of trainable (tunable) parameters: 2807010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def train_model(epochs:int, train_loader, model, optimizer, criterion):\n",
        "  model.train()\n",
        "  train_loss_per_epoch = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    running_loss_train = 0.0\n",
        "\n",
        "    for data_batch, target_batch in train_loader:\n",
        "\n",
        "        data_batch = data_batch.to('cpu')\n",
        "        target_batch = target_batch.to('cpu')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data_batch)\n",
        "        loss_train = criterion(output, target_batch)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        running_loss_train += loss_train.item()\n",
        "\n",
        "    epcoh_loss_train = running_loss_train / len(train_loader)\n",
        "\n",
        "    train_loss_per_epoch.append(epcoh_loss_train)\n",
        "\n",
        "\n",
        "    if epoch %2 == 0:\n",
        "      print(f'Epoch {epoch} Train loss {epcoh_loss_train}')\n",
        "  return train_loss_per_epoch\n",
        "\n",
        "\n",
        "def test(test_loader, net):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    wrong_counts = [0 for i in range(10)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            x, y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            output = net(x)\n",
        "            for idx, i in enumerate(output):\n",
        "                if torch.argmax(i) == y[idx]:\n",
        "                    correct +=1\n",
        "                else:\n",
        "                    wrong_counts[y[idx]] +=1\n",
        "                total +=1\n",
        "    print(f'Accuracy: {round(correct/total, 3)}')\n",
        "    for i in range(len(wrong_counts)):\n",
        "        print(f'wrong counts for the Label {i}: {wrong_counts[i]}')\n"
      ],
      "metadata": {
        "id": "KwkoL8hcFAU6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainLoss = train_model(epochs=10,\n",
        "#                         train_loader=train_loader_1,\n",
        "#                         optimizer = optimizer,\n",
        "#                         criterion=criterion,\n",
        "#                         model=BigNet,\n",
        "#                         )\n",
        "\n",
        "testloss = test(test_loader=test_loader,\n",
        "                        net=BigNet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UdLDwu_Ffr4",
        "outputId": "937a4a9f-1b3b-4158-e894-4f05387fa050"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.437\n",
            "wrong counts for the Label 0: 94\n",
            "wrong counts for the Label 1: 47\n",
            "wrong counts for the Label 2: 197\n",
            "wrong counts for the Label 3: 126\n",
            "wrong counts for the Label 4: 165\n",
            "wrong counts for the Label 5: 1000\n",
            "wrong counts for the Label 6: 1000\n",
            "wrong counts for the Label 7: 1000\n",
            "wrong counts for the Label 8: 1000\n",
            "wrong counts for the Label 9: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LightFashionNet(nn.Module):\n",
        "  def __init__(self, input_dims, num_class, input_hidden_size: 1000):\n",
        "\n",
        "    super().__init__()\n",
        "    self.input_dims = input_dims\n",
        "    self.num_class = num_class\n",
        "    self.input_hidden_size = input_hidden_size\n",
        "\n",
        "\n",
        "    self.input_layer = nn.Linear(in_features=self.input_dims, out_features=self.input_hidden_size)\n",
        "    self.output_layer = nn.Linear(in_features=self.input_hidden_size, out_features=self.num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.input_layer(x))\n",
        "    out = self.output_layer(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "-iO9JDVaFlSu"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LitNet = LightFashionNet(input_dims=784, input_hidden_size=1000, num_class=len(class_names))"
      ],
      "metadata": {
        "id": "6eb3Kqw2Krhc"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the norm of the first layer of the initial lightweight model\n",
        "print(\"Norm of 1st layer of nn_light:\", torch.norm(BigNet.input_layer.weight, p=2).item())\n",
        "# Print the norm of the first layer of the new lightweight model\n",
        "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(LitNet.input_layer.weight, p=2).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NQBC487MXYm",
        "outputId": "5fbb064d-bc72-4d4d-e7aa-5a55b308daf3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm of 1st layer of nn_light: 18.362703323364258\n",
            "Norm of 1st layer of new_nn_light: 18.259302139282227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LitNet.input_layer.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPrnsMnaMqcE",
        "outputId": "f0911ea0-a379-489f-b6ab-474570c3dd30"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0234, -0.0337, -0.0106,  ...,  0.0163, -0.0135,  0.0131],\n",
              "        [-0.0133,  0.0138,  0.0236,  ..., -0.0180,  0.0176,  0.0301],\n",
              "        [-0.0200,  0.0197, -0.0341,  ..., -0.0216, -0.0162,  0.0121],\n",
              "        ...,\n",
              "        [ 0.0126,  0.0313, -0.0097,  ..., -0.0266,  0.0335,  0.0263],\n",
              "        [ 0.0333,  0.0325,  0.0119,  ..., -0.0336, -0.0029, -0.0208],\n",
              "        [-0.0088,  0.0121, -0.0318,  ..., -0.0183, -0.0084,  0.0319]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_deep = \"{:,}\".format(sum(p.numel() for p in BigNet.parameters()))\n",
        "print(f\"DeepNN parameters: {total_params_deep}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in LitNet.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH29NR7XNnEP",
        "outputId": "b53ec364-27e2-4265-c35d-07f11ef7f7b5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepNN parameters: 2,807,010\n",
            "LightNN parameters: 795,010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainLoss = train_model(epochs=10,\n",
        "                        train_loader=train_loader_1,\n",
        "                        optimizer = optimizer,\n",
        "                        criterion=criterion,\n",
        "                        model=LitNet,\n",
        "                        )\n",
        "\n",
        "testloss = test(test_loader=test_loader,\n",
        "                        net=LitNet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PenXHREtOyMY",
        "outputId": "fcb7377e-497a-4f51-f5ba-94a5a6c814ad"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train loss 2.2553637434424623\n",
            "Epoch 2 Train loss 2.2553756562377343\n",
            "Epoch 4 Train loss 2.2553685662080483\n",
            "Epoch 6 Train loss 2.255374561240678\n",
            "Epoch 8 Train loss 2.2553869757825122\n",
            "Accuracy: 0.124\n",
            "wrong counts for the Label 0: 988\n",
            "wrong counts for the Label 1: 999\n",
            "wrong counts for the Label 2: 146\n",
            "wrong counts for the Label 3: 648\n",
            "wrong counts for the Label 4: 1000\n",
            "wrong counts for the Label 5: 1000\n",
            "wrong counts for the Label 6: 1000\n",
            "wrong counts for the Label 7: 1000\n",
            "wrong counts for the Label 8: 979\n",
            "wrong counts for the Label 9: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kd(teacher_model, student_model,  train_loader, epochs, criterion, T, device):\n",
        "\n",
        "  optimizer = optim.Adam(student_model.parameters(), lr=0.0001)\n",
        "\n",
        "  # set teacher model to eval mode\n",
        "  teacher_model.eval()\n",
        "  student_model.train()\n",
        "\n",
        "  train_loss_per_epoch = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    running_loss_train = 0.0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits_teacher = teacher_model(data)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      student_logits = student_model(data)\n",
        "      # Soften the student logits by applying softmax first and log() second\n",
        "\n",
        "      soft_targets = nn.functional.softmax(logits_teacher / T, dim=-1)\n",
        "      soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "      soft_targets_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0] * (T**2)\n",
        "\n",
        "      label_loss = criterion(student_logits, target)\n",
        "      loss =  0.25*soft_targets_loss + 0.75*label_loss\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss_train+=loss.item()\n",
        "      epcoh_loss_train = running_loss_train / len(train_loader)\n",
        "\n",
        "    train_loss_per_epoch.append(epcoh_loss_train)\n",
        "\n",
        "    if epoch %2 == 0:\n",
        "      print(f'Epoch {epoch} Train loss {epcoh_loss_train}')\n",
        "  return train_loss_per_epoch\n"
      ],
      "metadata": {
        "id": "gKP_CezyPhCk"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_kd(teacher_model=BigNet,\n",
        "         student_model=LitNet,\n",
        "         train_loader=train_loader_1,\n",
        "         epochs=10, T= 1,\n",
        "         device='cpu',\n",
        "         criterion=nn.CrossEntropyLoss())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kQaaziMTq-y",
        "outputId": "590754e2-4e26-4271-e6b4-68628a495a4a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train loss 0.2986892391560174\n",
            "Epoch 2 Train loss 0.28374231033233693\n",
            "Epoch 4 Train loss 0.27360067988382475\n",
            "Epoch 6 Train loss 0.26608492116302823\n",
            "Epoch 8 Train loss 0.25810768062880296\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2986892391560174,\n",
              " 0.2905925167903209,\n",
              " 0.28374231033233693,\n",
              " 0.27886686715553566,\n",
              " 0.27360067988382475,\n",
              " 0.269568753680949,\n",
              " 0.26608492116302823,\n",
              " 0.26121315055056166,\n",
              " 0.25810768062880296,\n",
              " 0.2543109116047176]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testloss = test(test_loader=test_loader,\n",
        "                        net=LitNet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPdYvDdBUDm4",
        "outputId": "d40d02bd-43bb-4fd6-b164-c343c4158f0a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.451\n",
            "wrong counts for the Label 0: 87\n",
            "wrong counts for the Label 1: 30\n",
            "wrong counts for the Label 2: 161\n",
            "wrong counts for the Label 3: 82\n",
            "wrong counts for the Label 4: 129\n",
            "wrong counts for the Label 5: 1000\n",
            "wrong counts for the Label 6: 1000\n",
            "wrong counts for the Label 7: 1000\n",
            "wrong counts for the Label 8: 1000\n",
            "wrong counts for the Label 9: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_kd(teacher_model=BigNet,\n",
        "         student_model=LitNet,\n",
        "         train_loader=train_loader_2,\n",
        "         epochs=10, T= 1,\n",
        "         device='cpu',\n",
        "         criterion=nn.CrossEntropyLoss())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkgjE_3PZEPR",
        "outputId": "869e5cfa-b571-4bf6-8369-f8e492b869d7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train loss 1.1719561132795013\n",
            "Epoch 2 Train loss 0.9092024168226002\n",
            "Epoch 4 Train loss 0.8917039465040032\n",
            "Epoch 6 Train loss 0.88123553648178\n",
            "Epoch 8 Train loss 0.873365246537906\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1719561132795013,\n",
              " 0.9263361792828737,\n",
              " 0.9092024168226002,\n",
              " 0.8991554599326811,\n",
              " 0.8917039465040032,\n",
              " 0.8862633935169879,\n",
              " 0.88123553648178,\n",
              " 0.8771729211309063,\n",
              " 0.873365246537906,\n",
              " 0.8697663337182897]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "testloss = test(test_loader=test_loader,\n",
        "                        net=LitNet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2KYtmmOZJlx",
        "outputId": "3f29b93c-e4bd-4453-8509-a810dcca6afc"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.594\n",
            "wrong counts for the Label 0: 995\n",
            "wrong counts for the Label 1: 122\n",
            "wrong counts for the Label 2: 987\n",
            "wrong counts for the Label 3: 777\n",
            "wrong counts for the Label 4: 994\n",
            "wrong counts for the Label 5: 53\n",
            "wrong counts for the Label 6: 18\n",
            "wrong counts for the Label 7: 43\n",
            "wrong counts for the Label 8: 27\n",
            "wrong counts for the Label 9: 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uBUvWAeWZ7DS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}