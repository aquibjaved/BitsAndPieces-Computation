{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNOtJ8XcyjZ0ezGv8ro7LhF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquibjaved/BitsAndPieces-Computation/blob/main/long_seq_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "kvcRp1pjUyND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizerFast\n",
        "import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "from transformers import DistilBertModel"
      ],
      "metadata": {
        "id": "vOdCFHu4Ndso"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the configuration for DistilBERT\n",
        "config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOX8QmuuNsUf",
        "outputId": "d960989c-7a26-45a6-f668-7ad51166be9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "seq_len = 700  # Input can be longer than max_seq_len (512)\n",
        "vocab_size = 30522\n",
        "\n",
        "# Generate valid input IDs\n",
        "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to('cuda')\n",
        "\n",
        "# Forward pass\n",
        "outputs, hidden_states = model(input_ids, return_hidden_states=True)\n",
        "print(hidden_states.shape)  # Expected: (batch_size, 512, 768)\n",
        "print(outputs.shape)  # Expected: (batch_size, num_classes\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUlU4W0GN1Za",
        "outputId": "35d81c5b-a29b-4e2a-f7d0-fce491d70919"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 768])\n",
            "torch.Size([2, 2])\n",
            "tensor([[-0.1684,  0.1270],\n",
            "        [-0.2044, -0.1317]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Trainable Parameters and Layers:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"Layer: {name} | Size: {param.size()} | Requires Grad: {param.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPMKFIPqN6f4",
        "outputId": "0c319a2a-3c1d-495d-eff6-c2987e745b04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters and Layers:\n",
            "Layer: embeddings.word_embeddings.weight | Size: torch.Size([30522, 768]) | Requires Grad: True\n",
            "Layer: embeddings.position_embeddings.weight | Size: torch.Size([512, 768]) | Requires Grad: True\n",
            "Layer: embeddings.LayerNorm.weight | Size: torch.Size([768]) | Requires Grad: True\n",
            "Layer: embeddings.LayerNorm.bias | Size: torch.Size([768]) | Requires Grad: True\n",
            "Layer: embedding.weight | Size: torch.Size([30522, 768]) | Requires Grad: True\n",
            "Layer: linear_reduce.weight | Size: torch.Size([768, 768]) | Requires Grad: True\n",
            "Layer: linear_reduce.bias | Size: torch.Size([768]) | Requires Grad: True\n",
            "Layer: attention.weight | Size: torch.Size([1, 768]) | Requires Grad: True\n",
            "Layer: attention.bias | Size: torch.Size([1]) | Requires Grad: True\n",
            "Layer: classifier.weight | Size: torch.Size([2, 768]) | Requires Grad: True\n",
            "Layer: classifier.bias | Size: torch.Size([2]) | Requires Grad: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate and print parameter statistics\n",
        "def print_trainable_params_percentage(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())  # Total parameters\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  # Trainable parameters\n",
        "\n",
        "    # Calculate percentage\n",
        "    percentage = (trainable_params / total_params) * 100 if total_params > 0 else 0\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"Total Parameters: {total_params}\")\n",
        "    print(f\"Trainable Parameters: {trainable_params}\")\n",
        "    print(f\"Percentage of Trainable Parameters: {percentage:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ESTzCsX5ORcu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_params_percentage(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8KIUqtiOjAR",
        "outputId": "538344ef-0e2f-48f0-f2d9-839b19a3c492"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 90396675\n",
            "Trainable Parameters: 47869443\n",
            "Percentage of Trainable Parameters: 52.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb = datasets.load_dataset(\"imdb\")\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "J4sgCkIDV333"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import DistilBertTokenizerFast\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load IMDB dataset\n",
        "imdb = load_dataset(\"imdb\")\n",
        "\n",
        "# Load DistilBERT tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize the training dataset and calculate sequence lengths\n",
        "train_texts = imdb['train']['text']\n",
        "tokenized_lengths = [len(tokenizer.encode(text, truncation=False, add_special_tokens=True)) for text in train_texts]\n",
        "\n",
        "# Plot the distribution of tokenized lengths\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(tokenized_lengths, bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Distribution of Tokenized Sequence Lengths in IMDB Training Dataset\", fontsize=16)\n",
        "plt.xlabel(\"Number of Tokens\", fontsize=14)\n",
        "plt.ylabel(\"Frequency\", fontsize=14)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print basic statistics\n",
        "print(\"Basic Statistics:\")\n",
        "print(f\"Minimum length: {min(tokenized_lengths)} tokens\")\n",
        "print(f\"Maximum length: {max(tokenized_lengths)} tokens\")\n",
        "print(f\"Average length: {sum(tokenized_lengths) / len(tokenized_lengths):.2f} tokens\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "collapsed": true,
        "id": "t51LeIpFWNWD",
        "outputId": "27cc033d-3d65-41e7-c724-3ead4d660653"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-cdf6b84fdbc1>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Tokenize the training dataset and calculate sequence lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtrain_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokenized_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Plot the distribution of tokenized lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-cdf6b84fdbc1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Tokenize the training dataset and calculate sequence lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtrain_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokenized_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Plot the distribution of tokenized lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2625\u001b[0m                 method).\n\u001b[1;32m   2626\u001b[0m         \"\"\"\n\u001b[0;32m-> 2627\u001b[0;31m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[1;32m   2628\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3044\u001b[0m         )\n\u001b[1;32m   3045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3046\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   3047\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3048\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     ) -> BatchEncoding:\n\u001b[1;32m    599\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "bJF01XoRWQtu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = imdb['train']['text']\n",
        "train_labels = imdb['train']['label']\n",
        "\n",
        "test_texts = imdb['test']['text']\n",
        "test_labels = imdb['test']['label']\n",
        "\n",
        "\n",
        "max_len = 700\n",
        "batch_size = 64\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len)\n",
        "test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_len)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "MV1MNebEfIjp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "a4juah4ytjxF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device, num_epochs):\n",
        "    \"\"\"\n",
        "    Train the model for a specified number of epochs.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to train.\n",
        "        dataloader (DataLoader): The DataLoader for training data.\n",
        "        optimizer (Optimizer): The optimizer for gradient descent.\n",
        "        criterion (Loss): The loss function.\n",
        "        device (torch.device): The device (CPU or GPU) for training.\n",
        "        num_epochs (int): The number of epochs to train.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] starting...\")\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # Use tqdm to wrap the dataloader for a progress bar\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move data to the device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy for this batch\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct_predictions += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            # Update progress bar description\n",
        "            progress_bar.set_postfix(\n",
        "                Loss=f\"{loss.item():.4f}\",\n",
        "                AvgLoss=f\"{total_loss / (batch_idx + 1):.4f}\"\n",
        "            )\n",
        "\n",
        "        # Calculate and log epoch accuracy\n",
        "        accuracy = correct_predictions / total_samples\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] completed. Average Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}\\n\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Move data to the device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "tCZ3IMiBhzrb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MergedDistilBertModel(DistilBertModel):\n",
        "    def __init__(self, config, vocab_size=30522, embed_dim=768, max_seq_len=512, padding_idx=0, num_classes=2):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Layers from SequenceReducer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "        self.linear_reduce = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attention = nn.Linear(embed_dim, 1)\n",
        "        self.pooling = nn.AdaptiveAvgPool2d((max_seq_len, embed_dim))\n",
        "\n",
        "        # Freeze all DistilBERT parameters\n",
        "        for param in self.transformer.parameters():  # Only freeze transformer weights\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, return_hidden_states=False, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass combining SequenceReducer and CustomDistilBertModel.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Token IDs of shape (batch_size, seq_len).\n",
        "            attention_mask (torch.Tensor): Optional attention mask of shape (batch_size, seq_len).\n",
        "            return_hidden_states (bool): Whether to return hidden states.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Logits for classification of shape (batch_size, num_classes).\n",
        "            Optional[torch.Tensor]: Hidden states of shape (batch_size, max_seq_len, embed_dim) if `return_hidden_states` is True.\n",
        "        \"\"\"\n",
        "        # SequenceReducer functionality\n",
        "        embeddings = self.embedding(input_ids)  # Shape: (batch_size, seq_len, embed_dim)\n",
        "        attention_scores = self.attention(embeddings)  # Shape: (batch_size, seq_len, 1)\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)  # Normalize scores\n",
        "        attended_embeddings = embeddings * attention_weights  # Shape: (batch_size, seq_len, embed_dim)\n",
        "        reduced_embeddings = self.linear_reduce(attended_embeddings)  # Shape: (batch_size, seq_len, embed_dim)\n",
        "        pooled_embeddings = self.pooling(reduced_embeddings.transpose(1, 2))  # Shape: (batch_size, max_seq_len, embed_dim)\n",
        "\n",
        "        # Pass reduced embeddings through DistilBERT (CustomDistilBertModel functionality)\n",
        "        position_ids = torch.arange(pooled_embeddings.size(1), dtype=torch.long, device=pooled_embeddings.device).unsqueeze(0)\n",
        "        position_embeddings = self.embeddings.position_embeddings(position_ids)\n",
        "\n",
        "        # Apply position embeddings and normalization/dropout layers\n",
        "        embeddings = pooled_embeddings + position_embeddings\n",
        "        embeddings = self.embeddings.LayerNorm(embeddings)\n",
        "        embeddings = self.embeddings.dropout(embeddings)\n",
        "\n",
        "        # print(\"Input Embeddings shape: \", embeddings.shape)\n",
        "        # print(\"attention_mask shape: \", attention_mask.shape)\n",
        "\n",
        "        # DistilBERT forward pass\n",
        "        hidden_states = super().forward(inputs_embeds=embeddings, attention_mask=attention_mask, **kwargs).last_hidden_state\n",
        "\n",
        "        # Classification head: Take [CLS]-like token (first token) for classification\n",
        "        cls_token = hidden_states[:, 0, :]  # Shape: (batch_size, embed_dim)\n",
        "        logits = self.classifier(cls_token)  # Shape: (batch_size, num_classes)\n",
        "\n",
        "        if return_hidden_states:\n",
        "            return logits, hidden_states\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "nMiGGpCEmxlM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MergedDistilBertModel(config).to('cuda')\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_epochs = 3\n",
        "\n",
        "# Define optimizer and loss\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# def train(model, dataloader, optimizer, criterion, device):\n",
        "train(model, train_dataloader, optimizer, criterion, device, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fnG5maDo4_i",
        "outputId": "54124a29-0a16-45fb-f634-7ff6a06a120d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/3] completed. Average Loss = 0.7052, Accuracy = 0.5051\n",
            "\n",
            "Epoch [3/3] starting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/3]:   1%|          | 3/391 [00:07<16:54,  2.62s/it, AvgLoss=0.6933, Loss=0.7142]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8UZD0ENimXWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4W1Ton7Fk4df"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}